---
---

@article{sedler2023lfads,
  bibtex_show={false},
  abbr={In Preparation},
  title={lfads-torch: A modular and extensible implementation of latent factor analysis via dynamical systems},
  author={Sedler, Andrew R and Pandarinath, Chethan},
  abstract={Latent factor analysis via dynamical systems (LFADS) is a variational sequential autoencoder that achieves state-of-the-art performance in denoising high-dimensional neural spiking activity for downstream applications in science and engineering. Recently introduced variants have continued to demonstrate the applicability of the architecture to a wide variety of problems in neuroscience. Since the development of the original implementation of LFADS, new technologies have emerged that use dynamic computation graphs, minimize boilerplate code, compose model configuration files, and simplify large-scale training. Building on these modern Python libraries, we introduce lfads-torch---a new open-source implementation of LFADS designed to be easier to understand, configure, and extend.},
  _journal={},
  _volume={},
  _number={},
  _pages={},
  year={2023},
  _doi={},
  _url={},
  _pdf={},
  _publisher={},
  selected={true},
}

@article{zhu2021deep,
  bibtex_show={true},
  abbr={NeurIPS},
  title={Deep inference of latent dynamics with spatio-temporal super-resolution using selective backpropagation through time},
  author={Zhu*, Feng and Sedler*, Andrew R and Grier, Harrison A and Ahad, Nauman and Davenport, Mark and Kaufman, Matthew and Giovannucci, Andrea and Pandarinath, Chethan},
  abstract={Modern neural interfaces allow access to the activity of up to a million neurons within brain circuits. However, bandwidth limits often create a trade-off between greater spatial sampling (more channels or pixels) and the temporal frequency of sampling. Here we demonstrate that it is possible to obtain spatio-temporal super-resolution in neuronal time series by exploiting relationships among neurons, embedded in latent low-dimensional population dynamics. Our novel neural network training strategy, selective backpropagation through time (SBTT), enables learning of deep generative models of latent dynamics from data in which the set of observed variables changes at each time step. The resulting models are able to infer activity for missing samples by combining observations with learned latent dynamics. We test SBTT applied to sequential autoencoders and demonstrate more efficient and higher-fidelity characterization of neural population dynamics in electrophysiological and calcium imaging data. In electrophysiology, SBTT enables accurate inference of neuronal population dynamics with lower interface bandwidths, providing an avenue to significant power savings for implanted neuroelectronic interfaces. In applications to two-photon calcium imaging, SBTT accurately uncovers high-frequency temporal structure underlying neural population activity, substantially outperforming the current state-of-the-art. Finally, we demonstrate that performance could be further improved by using limited, highbandwidth sampling to pretrain dynamics models, and then using SBTT to adapt these models for sparsely-sampled data.},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={2331--2345},
  year={2021},
  _doi={},
  _url={},
  html={https://proceedings.neurips.cc/paper/2021/hash/1325cdae3b6f0f91a1b629307bf2d498-Abstract.html},
  pdf={https://proceedings.neurips.cc/paper/2021/file/1325cdae3b6f0f91a1b629307bf2d498-Paper.pdf},
  _altimetric={},
  _preview={},
  selected={true},
}

@article{keshtkaran2021large,
  bibtex_show={true},
  abbr={Nat. Methods},
  title={A large-scale neural network training framework for generalized estimation of single-trial population dynamics},
  author={Keshtkaran*, Mohammad Reza and Sedler*, Andrew R and Chowdhury, Raeed H and Tandon, Raghav and Basrai, Diya and Nguyen, Sarah L and Sohn, Hansem and Jazayeri, Mehrdad and Miller, Lee E and Pandarinath, Chethan},
  abstract={Achieving state-of-the-art performance with deep neural population dynamics models requires extensive hyperparameter tuning for each dataset. AutoLFADS is a model-tuning framework that automatically produces high-performing autoencoding models on data from a variety of brain areas and tasks, without behavioral or task information. We demonstrate its broad applicability on several rhesus macaque datasets: from motor cortex during free-paced reaching, somatosensory cortex during reaching with perturbations, and dorsomedial frontal cortex during a cognitive timing task.},
  journal={Nature Methods},
  volume={19},
  number={12},
  pages={1572--1577},
  year={2022},
  publisher={Springer Nature},
  doi={10.1038/s41592-022-01675-0},
  url={https://doi.org/10.1038/s41592-022-01675-0},
  html={https://www.nature.com/articles/s41592-022-01675-0},
  pdf={https://rdcu.be/c179M},
  altmetric={139363169},
  _preview={lfads-schematic.png},
  selected={true},
}

@article{sedler2023expressive,
  bibtex_show={true},
  abbr={NBDT},
  title={Expressive architectures enhance interpretability of dynamics-based neural population models},
  author={Sedler, Andrew R and Versteeg, Christopher and Pandarinath, Chethan},
  abstract={Artificial neural networks that can recover latent dynamics from recorded neural activity may provide a powerful avenue for identifying and interpreting the dynamical motifs underlying biological computation. Given that neural variance alone does not uniquely determine a latent dynamical system, interpretable architectures should prioritize accurate and low-dimensional latent dynamics. In this work, we evaluated the performance of sequential autoencoders (SAEs) in recovering three latent chaotic attractors from simulated neural datasets. We found that SAEs with widely-used recurrent neural network (RNN)-based dynamics were unable to infer accurate rates at the true latent state dimensionality, and that larger RNNs relied upon dynamical features not present in the data. On the other hand, SAEs with neural ordinary differential equation (NODE)-based dynamics inferred accurate rates at the true latent state dimensionality, while also recovering latent trajectories and fixed point structure. We attribute this finding to the fact that NODEs allow use of multi-layer perceptrons (MLPs) of arbitrary capacity to model the vector field. Decoupling the expressivity of the dynamics model from its latent dimensionality enables NODEs to learn the requisite low-D dynamics where RNN cells fail. The suboptimal interpretability of widely-used RNN-based dynamics may motivate substitution for alternative architectures, such as NODE, that enable learning of accurate dynamics in low-dimensional latent spaces.},
  journal={Neurons, Behavior, Data analysis, and Theory},
  year={2023},
  doi={10.51628/001c.73987},
  url={https://doi.org/10.51628/001c.73987},
  html={https://arxiv.org/abs/2212.03771},
  pdf={https://arxiv.org/pdf/2212.03771.pdf},
  selected={true},
}

@article{karpowicz2022stabilizing,
  bibtex_show={true},
  abbr={bioRxiv},
  title={Stabilizing brain-computer interfaces through alignment of latent dynamics},
  author={Karpowicz, Brianna M and Ali, Yahia H and Wimalasena, Lahiru N and Sedler, Andrew R and Keshtkaran, Mohammad Reza and Bodkin, Kevin and Ma, Xuan and Miller, Lee E and Pandarinath, Chethan},
  abstract={Intracortical brain-computer interfaces (iBCIs) restore motor function to people with paralysis by translating brain activity into control signals for external devices. In current iBCIs, instabilities at the neural interface result in a degradation of decoding performance, which necessitates frequent supervised recalibration using new labeled data. One potential solution is to use the latent manifold structure that underlies neural population activity to facilitate a stable mapping between brain activity and behavior. Recent efforts using unsupervised approaches have improved iBCI stability using this principle; however, existing methods treat each time step as an independent sample and do not account for latent dynamics. Dynamics have been used to enable high performance prediction of movement intention, and may also help improve stabilization. Here, we present a platform for Nonlinear Manifold Alignment with Dynamics (NoMAD), which stabilizes iBCI decoding using recurrent neural network models of dynamics. NoMAD uses unsupervised distribution alignment to update the mapping of nonstationary neural data to a consistent set of neural dynamics, thereby providing stable input to the iBCI decoder. In applications to data from monkey motor cortex collected during motor tasks, NoMAD enables accurate behavioral decoding with unparalleled stability over weeks- to months-long timescales without any supervised recalibration.},
  journal={bioRxiv},
  year={2022},
  doi={10.1101/2022.04.06.487388v2},
  url={https://doi.org/10.1101/2022.04.06.487388},
  html={https://www.biorxiv.org/content/10.1101/2022.04.06.487388v2},
  pdf={https://www.biorxiv.org/content/10.1101/2022.04.06.487388v2.full.pdf},
  publisher={Cold Spring Harbor Laboratory}
}

@article{patel2023highperformance,
  bibtex_show={true},
  abbr={JOSS},
  title={High-performance neural population dynamics modeling enabled by scalable computational infrastructure},
  author={Patel*, Aashish N and Sedler*, Andrew R and Huang, Jingya and Pandarinath, Chethan and Gilja, Vikash},
  abstract={Advances in neural interface technology are facilitating parallel, high-dimensional time series measurements of the brain in action. A powerful strategy for analyzing these measurements is to apply unsupervised learning techniques to uncover lower-dimensional latent dynamics that explain much of the variance in the high-dimensional measurements. Latent factor analysis via dynamical systems (LFADS) provides a deep learning approach for extracting estimates of these latent dynamics from neural population data. The recently developed AutoLFADS framework extends LFADS by using Population Based Training (PBT) to effectively and scalably tune model hyperparameters, a critical step for accurate modeling of neural population data. As hyperparameter sweeps are one of the most computationally demanding processes in model development, these workflows should be deployed in a computationally efficient and cost effective manner given the compute resources available (e.g., local, institutionally-supported, or commercial computing clusters). The initial implementation of AutoLFADS used the Ray library to enable support for specific local and commercial cloud workflows. We extend this support, by providing additional options for training AutoLFADS models using local clusters in a container-native approach (e.g., Docker, Podman), unmanaged compute clusters leveraging Ray, and managed compute clusters leveraging KubeFlow and Kubernetes orchestration. As the neurosciences increasingly employ deep learning based models that require compute intensive hyperparameter optimization, standardization and dissemination of computational methods becomes increasingly challenging. Although this work specifically provides implementations of AutoLFADS, the tooling provided demonstrates strategies for employing computation at scale while facilitating dissemination and reproducibility.},
  journal={Journal of Open Source Software},
  volume={8},
  number={83},
  pages={5023},
  year={2023},
  doi={10.21105/joss.05023},
  url={https://doi.org/10.21105/joss.05023},
  pdf={https://joss.theoj.org/papers/10.21105/joss.05023.pdf},
  publisher={The Open Journal},
}


