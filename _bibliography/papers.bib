---
---

@article{keshtkaran2021large,
  bibtex_show={true},
  abbr={Nat. Methods},
  title={A large-scale neural network training framework for generalized estimation of single-trial population dynamics},
  author={Keshtkaran*, Mohammad Reza and Sedler*, Andrew R and Chowdhury, Raeed H and Tandon, Raghav and Basrai, Diya and Nguyen, Sarah L and Sohn, Hansem and Jazayeri, Mehrdad and Miller, Lee E and Pandarinath, Chethan},
  abstract={Achieving state-of-the-art performance with deep neural population dynamics models requires extensive hyperparameter tuning for each dataset. AutoLFADS is a model-tuning framework that automatically produces high-performing autoencoding models on data from a variety of brain areas and tasks, without behavioral or task information. We demonstrate its broad applicability on several rhesus macaque datasets: from motor cortex during free-paced reaching, somatosensory cortex during reaching with perturbations, and dorsomedial frontal cortex during a cognitive timing task.},
  journal={Nature Methods},
  volume={19},
  number={12},
  pages={1572--1577},
  year={2022},
  publisher={Springer Nature},
  doi={10.1038/s41592-022-01675-0},
  url={https://doi.org/10.1038/s41592-022-01675-0},
  html={https://www.nature.com/articles/s41592-022-01675-0},
  pdf={https://rdcu.be/c179M},
  altmetric={139363169},
  _preview={lfads-schematic.png},
  selected={true},
}

@article{zhu2021deep,
  bibtex_show={true},
  abbr={NeurIPS},
  title={Deep inference of latent dynamics with spatio-temporal super-resolution using selective backpropagation through time},
  author={Zhu*, Feng and Sedler*, Andrew R and Grier, Harrison A and Ahad, Nauman and Davenport, Mark and Kaufman, Matthew and Giovannucci, Andrea and Pandarinath, Chethan},
  abstract={Modern neural interfaces allow access to the activity of up to a million neurons within brain circuits. However, bandwidth limits often create a trade-off between greater spatial sampling (more channels or pixels) and the temporal frequency of sampling. Here we demonstrate that it is possible to obtain spatio-temporal super-resolution in neuronal time series by exploiting relationships among neurons, embedded in latent low-dimensional population dynamics. Our novel neural network training strategy, selective backpropagation through time (SBTT), enables learning of deep generative models of latent dynamics from data in which the set of observed variables changes at each time step. The resulting models are able to infer activity for missing samples by combining observations with learned latent dynamics. We test SBTT applied to sequential autoencoders and demonstrate more efficient and higher-fidelity characterization of neural population dynamics in electrophysiological and calcium imaging data. In electrophysiology, SBTT enables accurate inference of neuronal population dynamics with lower interface bandwidths, providing an avenue to significant power savings for implanted neuroelectronic interfaces. In applications to two-photon calcium imaging, SBTT accurately uncovers high-frequency temporal structure underlying neural population activity, substantially outperforming the current state-of-the-art. Finally, we demonstrate that performance could be further improved by using limited, highbandwidth sampling to pretrain dynamics models, and then using SBTT to adapt these models for sparsely-sampled data.},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={2331--2345},
  year={2021},
  _doi={},
  _url={},
  html={https://proceedings.neurips.cc/paper/2021/hash/1325cdae3b6f0f91a1b629307bf2d498-Abstract.html},
  pdf={https://proceedings.neurips.cc/paper/2021/file/1325cdae3b6f0f91a1b629307bf2d498-Paper.pdf},
  _altimetric={},
  _preview={},
  selected={true},
}

@article{sedler2022expressive,
  bibtex_show={true},
  abbr={arXiv},
  title={Expressive architectures enhance interpretability of dynamics-based neural population models},
  author={Sedler, Andrew R and Versteeg, Christopher and Pandarinath, Chethan},
  abstract={Artificial neural networks that can recover latent dynamics from recorded neural activity may provide a powerful avenue for identifying and interpreting the dynamical motifs underlying biological computation. Given that neural variance alone does not uniquely determine a latent dynamical system, interpretable architectures should prioritize accurate and low-dimensional latent dynamics. In this work, we evaluated the performance of sequential autoencoders (SAEs) in recovering three latent chaotic attractors from simulated neural datasets. We found that SAEs with widely-used recurrent neural network (RNN)-based dynamics were unable to infer accurate rates at the true latent state dimensionality, and that larger RNNs relied upon dynamical features not present in the data. On the other hand, SAEs with neural ordinary differential equation (NODE)-based dynamics inferred accurate rates at the true latent state dimensionality, while also recovering latent trajectories and fixed point structure. We attribute this finding to the fact that NODEs allow use of multi-layer perceptrons (MLPs) of arbitrary capacity to model the vector field. Decoupling the expressivity of the dynamics model from its latent dimensionality enables NODEs to learn the requisite low-D dynamics where RNN cells fail. The suboptimal interpretability of widely-used RNN-based dynamics may motivate substitution for alternative architectures, such as NODE, that enable learning of accurate dynamics in low-dimensional latent spaces.},
  journal={arXiv preprint arXiv:2212.03771},
  year={2022},
  doi={10.48550/arXiv.2212.03771},
  url={https://doi.org/10.48550/arXiv.2212.03771},
  html={https://arxiv.org/abs/2212.03771},
  pdf={https://arxiv.org/pdf/2212.03771.pdf},
  selected={true},
}

@article{karpowicz2022stabilizing,
  bibtex_show={true},
  abbr={bioRxiv},
  title={Stabilizing brain-computer interfaces through alignment of latent dynamics},
  author={Karpowicz, Brianna M and Ali, Yahia H and Wimalasena, Lahiru N and Sedler, Andrew R and Keshtkaran, Mohammad Reza and Bodkin, Kevin and Ma, Xuan and Miller, Lee E and Pandarinath, Chethan},
  abstract={Intracortical brain-computer interfaces (iBCIs) restore motor function to people with paralysis by translating brain activity into control signals for external devices. In current iBCIs, instabilities at the neural interface result in a degradation of decoding performance, which necessitates frequent supervised recalibration using new labeled data. One potential solution is to use the latent manifold structure that underlies neural population activity to facilitate a stable mapping between brain activity and behavior. Recent efforts using unsupervised approaches have improved iBCI stability using this principle; however, existing methods treat each time step as an independent sample and do not account for latent dynamics. Dynamics have been used to enable high performance prediction of movement intention, and may also help improve stabilization. Here, we present a platform for Nonlinear Manifold Alignment with Dynamics (NoMAD), which stabilizes iBCI decoding using recurrent neural network models of dynamics. NoMAD uses unsupervised distribution alignment to update the mapping of nonstationary neural data to a consistent set of neural dynamics, thereby providing stable input to the iBCI decoder. In applications to data from monkey motor cortex collected during motor tasks, NoMAD enables accurate behavioral decoding with unparalleled stability over weeks- to months-long timescales without any supervised recalibration.},
  journal={bioRxiv},
  year={2022},
  doi={10.1101/2022.04.06.487388v2},
  url={https://doi.org/10.1101/2022.04.06.487388},
  html={https://www.biorxiv.org/content/10.1101/2022.04.06.487388v2},
  pdf={https://www.biorxiv.org/content/10.1101/2022.04.06.487388v2.full.pdf},
  publisher={Cold Spring Harbor Laboratory}
}
