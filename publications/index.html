<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>publications | Andrew R. Sedler</title> <meta name="author" content="Andrew R. Sedler"> <meta name="description" content="asterisk denotes equal contibution"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%A1&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://arsedler9.github.io/publications/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> </head> <body class="fixed-top-nav sticky-bottom-footer"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Andrew </span>R. Sedler</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item active"> <a class="nav-link" href="/publications/">publications<span class="sr-only">(current)</span></a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">publications</h1> <p class="post-description">asterisk denotes equal contibution</p> </header> <article> <div class="publications"> <h2 class="year">2023</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge"><a href="https://arxiv.org/" rel="external nofollow noopener" target="_blank">arXiv</a></abbr></div> <div id="sedler2023lfads" class="col-sm-8"> <div class="title">lfads-torch: A modular and extensible implementation of latent factor analysis via dynamical systems</div> <div class="author"> <em>Andrew R Sedler</em>, and <a href="https://snel.gatech.edu/pi/" rel="external nofollow noopener" target="_blank">Chethan Pandarinath</a> </div> <div class="periodical"> <em>arXiv</em> 2023 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/pdf/2309.01230.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/arsedler9/lfads-torch" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>Latent factor analysis via dynamical systems (LFADS) is an RNN-based variational sequential autoencoder that achieves state-of-the-art performance in denoising high-dimensional neural activity for downstream applications in science and engineering. Recently introduced variants and extensions continue to demonstrate the applicability of the architecture to a wide variety of problems in neuroscience. Since the development of the original implementation of LFADS, new technologies have emerged that use dynamic computation graphs, minimize boilerplate code, compose model configuration files, and simplify large-scale training. Building on these modern Python libraries, we introduce lfads-torch – a new open-source implementation of LFADS that unifies existing variants and is designed to be easier to understand, configure, and extend.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">sedler2023lfads</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{lfads-torch: A modular and extensible implementation of latent factor analysis via dynamical systems}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Sedler, Andrew R and Pandarinath, Chethan}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{arXiv}</span><span class="p">,</span>
  <span class="na">_volume</span> <span class="p">=</span> <span class="s">{}</span><span class="p">,</span>
  <span class="na">_number</span> <span class="p">=</span> <span class="s">{}</span><span class="p">,</span>
  <span class="na">_pages</span> <span class="p">=</span> <span class="s">{}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">_doi</span> <span class="p">=</span> <span class="s">{}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Cold Spring Harbor Laboratory}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge"><a href="https://arxiv.org/" rel="external nofollow noopener" target="_blank">arXiv</a></abbr></div> <div id="versteeg2023expressive" class="col-sm-8"> <div class="title">Expressive dynamics models with nonlinear injective readouts enable reliable recovery of latent features from neural activity</div> <div class="author"> Christopher Versteeg, <em>Andrew R Sedler</em>, Jonathan D McCart, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Chethan Pandarinath' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> <em>arXiv</em> 2023 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/pdf/2309.06402.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>The advent of large-scale neural recordings has enabled new methods to discover the computational mechanisms of neural circuits by understanding the rules that govern how their state evolves over time. While these \textitneural dynamics cannot be directly measured, they can typically be approximated by low-dimensional models in a latent space. How these models represent the mapping from latent space to neural space can affect the interpretability of the latent representation. We show that typical choices for this mapping (e.g., linear or MLP) often lack the property of injectivity, meaning that changes in latent state are not obligated to affect activity in the neural space. During training, non-injective readouts incentivize the invention of dynamics that misrepresent the underlying system and the computation it performs. Combining our injective Flow readout with prior work on interpretable latent dynamics models, we created the Ordinary Differential equations autoencoder with Injective Nonlinear readout (ODIN), which captures latent dynamical systems that are nonlinearly embedded into observed neural activity via an approximately injective nonlinear mapping. We show that ODIN can recover nonlinearly embedded systems from simulated neural activity, even when the nature of the system and embedding are unknown. Additionally, ODIN enables the unsupervised recovery of underlying dynamical features (e.g., fixed points) and embedding geometry. When applied to biological neural recordings, ODIN can reconstruct neural activity with comparable accuracy to previous state-of-the-art methods while using substantially fewer latent dimensions. Overall, ODIN’s accuracy in recovering ground-truth latent features and ability to accurately reconstruct neural activity with low dimensionality make it a promising method for distilling interpretable dynamics that can help explain neural computation.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">versteeg2023expressive</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Expressive dynamics models with nonlinear injective readouts enable reliable recovery of latent features from neural activity}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Versteeg, Christopher and Sedler, Andrew R and McCart, Jonathan D and Pandarinath, Chethan}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{arXiv}</span><span class="p">,</span>
  <span class="na">_volume</span> <span class="p">=</span> <span class="s">{}</span><span class="p">,</span>
  <span class="na">_number</span> <span class="p">=</span> <span class="s">{}</span><span class="p">,</span>
  <span class="na">_pages</span> <span class="p">=</span> <span class="s">{}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">_doi</span> <span class="p">=</span> <span class="s">{}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Cold Spring Harbor Laboratory}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge"><a href="https://nbdt.scholasticahq.com/" rel="external nofollow noopener" target="_blank">NBDT</a></abbr></div> <div id="sedler2023expressive" class="col-sm-8"> <div class="title">Expressive architectures enhance interpretability of dynamics-based neural population models</div> <div class="author"> <em>Andrew R Sedler</em>, Christopher Versteeg, and <a href="https://snel.gatech.edu/pi/" rel="external nofollow noopener" target="_blank">Chethan Pandarinath</a> </div> <div class="periodical"> <em>Neurons, Behavior, Data analysis, and Theory</em> 2023 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/abs/2212.03771" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://arxiv.org/pdf/2212.03771.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/snel-repo/expressive-latent-dynamics-paper" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>Artificial neural networks that can recover latent dynamics from recorded neural activity may provide a powerful avenue for identifying and interpreting the dynamical motifs underlying biological computation. Given that neural variance alone does not uniquely determine a latent dynamical system, interpretable architectures should prioritize accurate and low-dimensional latent dynamics. In this work, we evaluated the performance of sequential autoencoders (SAEs) in recovering three latent chaotic attractors from simulated neural datasets. We found that SAEs with widely-used recurrent neural network (RNN)-based dynamics were unable to infer accurate rates at the true latent state dimensionality, and that larger RNNs relied upon dynamical features not present in the data. On the other hand, SAEs with neural ordinary differential equation (NODE)-based dynamics inferred accurate rates at the true latent state dimensionality, while also recovering latent trajectories and fixed point structure. We attribute this finding to the fact that NODEs allow use of multi-layer perceptrons (MLPs) of arbitrary capacity to model the vector field. Decoupling the expressivity of the dynamics model from its latent dimensionality enables NODEs to learn the requisite low-D dynamics where RNN cells fail. The suboptimal interpretability of widely-used RNN-based dynamics may motivate substitution for alternative architectures, such as NODE, that enable learning of accurate dynamics in low-dimensional latent spaces.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">sedler2023expressive</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Expressive architectures enhance interpretability of dynamics-based neural population models}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Sedler, Andrew R and Versteeg, Christopher and Pandarinath, Chethan}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Neurons, Behavior, Data analysis, and Theory}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.51628/001c.73987}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://doi.org/10.51628/001c.73987}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge"><a href="https://joss.theoj.org/" rel="external nofollow noopener" target="_blank">JOSS</a></abbr></div> <div id="patel2023highperformance" class="col-sm-8"> <div class="title">High-performance neural population dynamics modeling enabled by scalable computational infrastructure</div> <div class="author"> Aashish N Patel*, <em>Andrew R Sedler*</em>, Jingya Huang, and <span class="more-authors" title="click to view 2 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '2 more authors' ? 'Chethan Pandarinath, Vikash Gilja' : '2 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">2 more authors</span> </div> <div class="periodical"> <em>Journal of Open Source Software</em> 2023 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://joss.theoj.org/papers/10.21105/joss.05023.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/TNEL-UCSD/autolfads-deploy" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>Advances in neural interface technology are facilitating parallel, high-dimensional time series measurements of the brain in action. A powerful strategy for analyzing these measurements is to apply unsupervised learning techniques to uncover lower-dimensional latent dynamics that explain much of the variance in the high-dimensional measurements. Latent factor analysis via dynamical systems (LFADS) provides a deep learning approach for extracting estimates of these latent dynamics from neural population data. The recently developed AutoLFADS framework extends LFADS by using Population Based Training (PBT) to effectively and scalably tune model hyperparameters, a critical step for accurate modeling of neural population data. As hyperparameter sweeps are one of the most computationally demanding processes in model development, these workflows should be deployed in a computationally efficient and cost effective manner given the compute resources available (e.g., local, institutionally-supported, or commercial computing clusters). The initial implementation of AutoLFADS used the Ray library to enable support for specific local and commercial cloud workflows. We extend this support, by providing additional options for training AutoLFADS models using local clusters in a container-native approach (e.g., Docker, Podman), unmanaged compute clusters leveraging Ray, and managed compute clusters leveraging KubeFlow and Kubernetes orchestration. As the neurosciences increasingly employ deep learning based models that require compute intensive hyperparameter optimization, standardization and dissemination of computational methods becomes increasingly challenging. Although this work specifically provides implementations of AutoLFADS, the tooling provided demonstrates strategies for employing computation at scale while facilitating dissemination and reproducibility.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">patel2023highperformance</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{High-performance neural population dynamics modeling enabled by scalable computational infrastructure}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Patel*, Aashish N and Sedler*, Andrew R and Huang, Jingya and Pandarinath, Chethan and Gilja, Vikash}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Journal of Open Source Software}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{8}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{83}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{5023}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.21105/joss.05023}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://doi.org/10.21105/joss.05023}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{The Open Journal}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="year">2022</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge"><a href="https://www.nature.com/nmeth/" rel="external nofollow noopener" target="_blank">Nat. Methods</a></abbr></div> <div id="keshtkaran2021large" class="col-sm-8"> <div class="title">A large-scale neural network training framework for generalized estimation of single-trial population dynamics</div> <div class="author"> <a href="https://www.linkedin.com/in/mrezak" rel="external nofollow noopener" target="_blank">Mohammad Reza Keshtkaran*</a>, <em>Andrew R Sedler*</em>, Raeed H Chowdhury, and <span class="more-authors" title="click to view 7 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '7 more authors' ? 'Raghav Tandon, Diya Basrai, Sarah L Nguyen, Hansem Sohn, Mehrdad Jazayeri, Lee E Miller, Chethan Pandarinath' : '7 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">7 more authors</span> </div> <div class="periodical"> <em>Nature Methods</em> 2022 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://www.nature.com/articles/s41592-022-01675-0" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://rdcu.be/c179M" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/snel-repo/autolfads-tf2" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <script type="text/javascript" src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script><div data-altmetric-id="139363169" class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-popover="right" data-badge-type="2" style="display:inline"></div> </div> <div class="abstract hidden"> <p>Achieving state-of-the-art performance with deep neural population dynamics models requires extensive hyperparameter tuning for each dataset. AutoLFADS is a model-tuning framework that automatically produces high-performing autoencoding models on data from a variety of brain areas and tasks, without behavioral or task information. We demonstrate its broad applicability on several rhesus macaque datasets: from motor cortex during free-paced reaching, somatosensory cortex during reaching with perturbations, and dorsomedial frontal cortex during a cognitive timing task.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">keshtkaran2021large</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{A large-scale neural network training framework for generalized estimation of single-trial population dynamics}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Keshtkaran*, Mohammad Reza and Sedler*, Andrew R and Chowdhury, Raeed H and Tandon, Raghav and Basrai, Diya and Nguyen, Sarah L and Sohn, Hansem and Jazayeri, Mehrdad and Miller, Lee E and Pandarinath, Chethan}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Nature Methods}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{19}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{12}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1572--1577}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Springer Nature}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1038/s41592-022-01675-0}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1038/s41592-022-01675-0}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge"><a href="https://www.biorxiv.org/" rel="external nofollow noopener" target="_blank">bioRxiv</a></abbr></div> <div id="karpowicz2022stabilizing" class="col-sm-8"> <div class="title">Stabilizing brain-computer interfaces through alignment of latent dynamics</div> <div class="author"> Brianna M Karpowicz, Yahia H Ali, Lahiru N Wimalasena, and <span class="more-authors" title="click to view 6 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '6 more authors' ? 'Andrew R Sedler, Mohammad Reza Keshtkaran, Kevin Bodkin, Xuan Ma, Lee E Miller, Chethan Pandarinath' : '6 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">6 more authors</span> </div> <div class="periodical"> <em>bioRxiv</em> 2022 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://www.biorxiv.org/content/10.1101/2022.04.06.487388v2" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://www.biorxiv.org/content/10.1101/2022.04.06.487388v2.full.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>Intracortical brain-computer interfaces (iBCIs) restore motor function to people with paralysis by translating brain activity into control signals for external devices. In current iBCIs, instabilities at the neural interface result in a degradation of decoding performance, which necessitates frequent supervised recalibration using new labeled data. One potential solution is to use the latent manifold structure that underlies neural population activity to facilitate a stable mapping between brain activity and behavior. Recent efforts using unsupervised approaches have improved iBCI stability using this principle; however, existing methods treat each time step as an independent sample and do not account for latent dynamics. Dynamics have been used to enable high performance prediction of movement intention, and may also help improve stabilization. Here, we present a platform for Nonlinear Manifold Alignment with Dynamics (NoMAD), which stabilizes iBCI decoding using recurrent neural network models of dynamics. NoMAD uses unsupervised distribution alignment to update the mapping of nonstationary neural data to a consistent set of neural dynamics, thereby providing stable input to the iBCI decoder. In applications to data from monkey motor cortex collected during motor tasks, NoMAD enables accurate behavioral decoding with unparalleled stability over weeks- to months-long timescales without any supervised recalibration.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">karpowicz2022stabilizing</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Stabilizing brain-computer interfaces through alignment of latent dynamics}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Karpowicz, Brianna M and Ali, Yahia H and Wimalasena, Lahiru N and Sedler, Andrew R and Keshtkaran, Mohammad Reza and Bodkin, Kevin and Ma, Xuan and Miller, Lee E and Pandarinath, Chethan}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{bioRxiv}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1101/2022.04.06.487388v2}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1101/2022.04.06.487388}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Cold Spring Harbor Laboratory}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="year">2021</h2> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge"><a href="https://neurips.cc/" rel="external nofollow noopener" target="_blank">NeurIPS</a></abbr></div> <div id="zhu2021deep" class="col-sm-8"> <div class="title">Deep inference of latent dynamics with spatio-temporal super-resolution using selective backpropagation through time</div> <div class="author"> <a href="https://www.linkedin.com/in/feng-zhu-304069aa/" rel="external nofollow noopener" target="_blank">Feng Zhu*</a>, <em>Andrew R Sedler*</em>, Harrison A Grier, and <span class="more-authors" title="click to view 5 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '5 more authors' ? 'Nauman Ahad, Mark Davenport, Matthew Kaufman, Andrea Giovannucci, Chethan Pandarinath' : '5 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">5 more authors</span> </div> <div class="periodical"> <em>Advances in Neural Information Processing Systems</em> 2021 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://proceedings.neurips.cc/paper/2021/hash/1325cdae3b6f0f91a1b629307bf2d498-Abstract.html" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://proceedings.neurips.cc/paper/2021/file/1325cdae3b6f0f91a1b629307bf2d498-Paper.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/snel-repo/sbtt-demo" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>Modern neural interfaces allow access to the activity of up to a million neurons within brain circuits. However, bandwidth limits often create a trade-off between greater spatial sampling (more channels or pixels) and the temporal frequency of sampling. Here we demonstrate that it is possible to obtain spatio-temporal super-resolution in neuronal time series by exploiting relationships among neurons, embedded in latent low-dimensional population dynamics. Our novel neural network training strategy, selective backpropagation through time (SBTT), enables learning of deep generative models of latent dynamics from data in which the set of observed variables changes at each time step. The resulting models are able to infer activity for missing samples by combining observations with learned latent dynamics. We test SBTT applied to sequential autoencoders and demonstrate more efficient and higher-fidelity characterization of neural population dynamics in electrophysiological and calcium imaging data. In electrophysiology, SBTT enables accurate inference of neuronal population dynamics with lower interface bandwidths, providing an avenue to significant power savings for implanted neuroelectronic interfaces. In applications to two-photon calcium imaging, SBTT accurately uncovers high-frequency temporal structure underlying neural population activity, substantially outperforming the current state-of-the-art. Finally, we demonstrate that performance could be further improved by using limited, highbandwidth sampling to pretrain dynamics models, and then using SBTT to adapt these models for sparsely-sampled data.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">zhu2021deep</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Deep inference of latent dynamics with spatio-temporal super-resolution using selective backpropagation through time}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Zhu*, Feng and Sedler*, Andrew R and Grier, Harrison A and Ahad, Nauman and Davenport, Mark and Kaufman, Matthew and Giovannucci, Andrea and Pandarinath, Chethan}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Advances in Neural Information Processing Systems}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{34}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{2331--2345}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">_doi</span> <span class="p">=</span> <span class="s">{}</span><span class="p">,</span>
  <span class="na">_url</span> <span class="p">=</span> <span class="s">{}</span><span class="p">,</span>
  <span class="na">_altimetric</span> <span class="p">=</span> <span class="s">{}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li></ol> </div> </article> </div> </div> <footer class="sticky-bottom mt-5"> <div class="container"> © Copyright 2023 Andrew R. Sedler. Last updated: September 15, 2023. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="/assets/js/common.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>